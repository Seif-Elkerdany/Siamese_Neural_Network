{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Set up notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import uuid\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, Input, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_Path = os.path.join(\"data\",\"positive\")\n",
    "NEG_Path = os.path.join(\"data\",\"negative\")\n",
    "ANC_Path = os.path.join(\"data\",\"anchor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dir\n",
    "os.makedirs(POS_Path)\n",
    "os.makedirs(NEG_Path)\n",
    "os.makedirs(ANC_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move LFW Images to the following repository data/negative\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "       \n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_Path, file)\n",
    "        os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Collecting pos and anchor data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to web cam and collecting my data imgs\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret , frame = cap.read()\n",
    "    \n",
    "    # Make the size of the frame to 250x250px\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    # Collecting ANC\n",
    "    if cv2.waitKey(1) & 0XFF == ord(\"a\"):\n",
    "        # Create unique file name \n",
    "        imgname = os.path.join(ANC_Path, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collecting POS\n",
    "    if cv2.waitKey(1) & 0XFF == ord(\"p\"):\n",
    "        imgname = os.path.join(POS_Path, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    cv2.imshow(\"Collecting data\",frame)\n",
    "    if cv2.waitKey(1) & 0XFF == ord(\"e\"):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images and preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 300 img\n",
    "\n",
    "anchor = tf.data.Dataset.list_files(ANC_Path+'\\*.jpg').take(300)\n",
    "positive = tf.data.Dataset.list_files(POS_Path+'\\*.jpg').take(300)\n",
    "negative = tf.data.Dataset.list_files(NEG_Path+'\\*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing img before entering the model\n",
    "\n",
    "def preprocessing(file_path):\n",
    "    # Read in image from file path and load it\n",
    "    byte_img = tf.io.read_file(file_path) \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (anchor, positive) ==> 1,1,1,1,1\n",
    "# (anchor, negative) ==> 0,0,0,0,0\n",
    "\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "\n",
    "data = positives.concatenate(negatives)\n",
    "samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_twin(input_img,validation_img,label):\n",
    "    return(preprocessing(input_img),preprocessing(validation_img),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = samples.next()\n",
    "\n",
    "# Plotting img and its label from the data\n",
    "res = preprocessing_twin(*example)\n",
    "plt.imshow(res[0]);\n",
    "print(res[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocessing_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data \n",
    "train_data = data.take(round(len(data)*0.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data\n",
    "testing_data = data.skip(round(len(data)*0.7))\n",
    "testing_data = testing_data.take(round(len(data)*0.3))\n",
    "testing_data = testing_data.batch(16)\n",
    "testing_data = testing_data.prefetch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Model engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inp = Input(shape=(100,100,3), name = \"input_image\")\n",
    "    \n",
    "    # First block\n",
    "    conv1 = Conv2D(64,(10,10),activation = \"relu\")(inp)\n",
    "    m1 = MaxPooling2D(64,(2,2),padding=\"same\")(conv1)\n",
    "    \n",
    "    # Second block\n",
    "    conv2 = Conv2D(128,(7,7),activation = \"relu\")(m1)\n",
    "    m2 = MaxPooling2D(64,(2,2),padding=\"same\")(conv2)\n",
    "    \n",
    "    # Third block\n",
    "    conv3 = Conv2D(128,(4,4),activation = \"relu\")(m2)\n",
    "    m3 = MaxPooling2D(64,(2,2),padding=\"same\")(conv3)\n",
    "    \n",
    "    # Final block\n",
    "    conv4 = Conv2D(256,(4,4),activation = \"relu\")(m3)\n",
    "    flat = Flatten()(conv4)\n",
    "    den = Dense(4096,activation = \"sigmoid\")(flat)\n",
    "    \n",
    "    return Model(inputs = [inp] ,outputs = [den] ,name = \"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 46, 46, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 40, 40, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 20, 20, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 17, 17, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 9, 9, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38960448 (148.62 MB)\n",
      "Trainable params: 38960448 (148.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "emb_model = make_embedding()\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1DIST(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "    # similarity calculations\n",
    "    def call(self, input_emb, validation_emb):\n",
    "        return tf.math.abs(input_emb - validation_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making siamese model\n",
    "def siamese_model():\n",
    "    \n",
    "    # Anchor img\n",
    "    input_img = Input(name = \"input_img\",shape = (100,100,3))\n",
    "    # validation img\n",
    "    validation_img = Input(name = \"validation_img\",shape = (100,100,3))\n",
    "    \n",
    "    # Siamese distance\n",
    "    siamese_layer = L1DIST()\n",
    "    siamese_layer.__name = \"distance\"\n",
    "    distance = siamese_layer(emb_model(input_img),emb_model(validation_img))\n",
    "    \n",
    "    # Classification layer\n",
    "    classifier = Dense(1,activation=\"sigmoid\")(distance)\n",
    "    \n",
    "    return Model(inputs = [input_img,validation_img],outputs = classifier,name = \"SiameseNetwork\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_img (InputLayer)      [(None, 100, 100, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " validation_img (InputLayer  [(None, 100, 100, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)      (None, 4096)                 3896044   ['input_img[0][0]',           \n",
      "                                                          8          'validation_img[0][0]']      \n",
      "                                                                                                  \n",
      " l1dist (L1DIST)             (None, 4096)                 0         ['embedding[0][0]',           \n",
      "                                                                     'embedding[1][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    4097      ['l1dist[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38964545 (148.64 MB)\n",
      "Trainable params: 38964545 (148.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "siamese_model_ = siamese_model()\n",
    "siamese_model_.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer \n",
    "opt = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training the model\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = train_data.as_numpy_iterator()\n",
    "batch1 = test_batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # get anchor, pos and negative img\n",
    "        X = batch[:2]\n",
    "        # get label\n",
    "        Y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = siamese_model_(X,training = True)\n",
    "        # Calc loss\n",
    "        loss = binary_cross_loss(Y,y_pred)\n",
    "    # Calc Gradients\n",
    "    grad = tape.gradient(loss,siamese_model_.trainable_variables)\n",
    "    \n",
    "    # Calc updated weights\n",
    "    opt.apply_gradients(zip(grad,siamese_model_.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,EPOCHS):\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progress_bar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx,batch in enumerate(data):\n",
    "            # Run train step\n",
    "            train_step(batch)\n",
    "            progress_bar.update(idx+1)\n",
    "        \n",
    "        # save checkpoints\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/50\n",
      "27/27 [==============================] - 18s 626ms/step\n",
      "\n",
      " Epoch 2/50\n",
      "27/27 [==============================] - 17s 633ms/step\n",
      "\n",
      " Epoch 3/50\n",
      "27/27 [==============================] - 17s 642ms/step\n",
      "\n",
      " Epoch 4/50\n",
      "27/27 [==============================] - 17s 640ms/step\n",
      "\n",
      " Epoch 5/50\n",
      "27/27 [==============================] - 17s 641ms/step\n",
      "\n",
      " Epoch 6/50\n",
      "27/27 [==============================] - 17s 637ms/step\n",
      "\n",
      " Epoch 7/50\n",
      "27/27 [==============================] - 17s 638ms/step\n",
      "\n",
      " Epoch 8/50\n",
      "27/27 [==============================] - 17s 634ms/step\n",
      "\n",
      " Epoch 9/50\n",
      "27/27 [==============================] - 17s 635ms/step\n",
      "\n",
      " Epoch 10/50\n",
      "27/27 [==============================] - 17s 636ms/step\n",
      "\n",
      " Epoch 11/50\n",
      "27/27 [==============================] - 17s 627ms/step\n",
      "\n",
      " Epoch 12/50\n",
      "27/27 [==============================] - 17s 639ms/step\n",
      "\n",
      " Epoch 13/50\n",
      "27/27 [==============================] - 17s 641ms/step\n",
      "\n",
      " Epoch 14/50\n",
      "27/27 [==============================] - 17s 642ms/step\n",
      "\n",
      " Epoch 15/50\n",
      "27/27 [==============================] - 17s 640ms/step\n",
      "\n",
      " Epoch 16/50\n",
      "27/27 [==============================] - 17s 642ms/step\n",
      "\n",
      " Epoch 17/50\n",
      "27/27 [==============================] - 17s 639ms/step\n",
      "\n",
      " Epoch 18/50\n",
      "27/27 [==============================] - 17s 644ms/step\n",
      "\n",
      " Epoch 19/50\n",
      "27/27 [==============================] - 17s 636ms/step\n",
      "\n",
      " Epoch 20/50\n",
      "27/27 [==============================] - 17s 637ms/step\n",
      "\n",
      " Epoch 21/50\n",
      "27/27 [==============================] - 17s 643ms/step\n",
      "\n",
      " Epoch 22/50\n",
      "27/27 [==============================] - 17s 640ms/step\n",
      "\n",
      " Epoch 23/50\n",
      "27/27 [==============================] - 17s 642ms/step\n",
      "\n",
      " Epoch 24/50\n",
      "27/27 [==============================] - 17s 637ms/step\n",
      "\n",
      " Epoch 25/50\n",
      "27/27 [==============================] - 17s 645ms/step\n",
      "\n",
      " Epoch 26/50\n",
      "27/27 [==============================] - 17s 646ms/step\n",
      "\n",
      " Epoch 27/50\n",
      "27/27 [==============================] - 17s 643ms/step\n",
      "\n",
      " Epoch 28/50\n",
      "27/27 [==============================] - 17s 635ms/step\n",
      "\n",
      " Epoch 29/50\n",
      "27/27 [==============================] - 17s 642ms/step\n",
      "\n",
      " Epoch 30/50\n",
      "27/27 [==============================] - 17s 637ms/step\n",
      "\n",
      " Epoch 31/50\n",
      "27/27 [==============================] - 17s 645ms/step\n",
      "\n",
      " Epoch 32/50\n",
      "27/27 [==============================] - 17s 647ms/step\n",
      "\n",
      " Epoch 33/50\n",
      "27/27 [==============================] - 17s 647ms/step\n",
      "\n",
      " Epoch 34/50\n",
      "27/27 [==============================] - 18s 647ms/step\n",
      "\n",
      " Epoch 35/50\n",
      "27/27 [==============================] - 18s 648ms/step\n",
      "\n",
      " Epoch 36/50\n",
      "27/27 [==============================] - 17s 646ms/step\n",
      "\n",
      " Epoch 37/50\n",
      "27/27 [==============================] - 18s 650ms/step\n",
      "\n",
      " Epoch 38/50\n",
      "27/27 [==============================] - 18s 650ms/step\n",
      "\n",
      " Epoch 39/50\n",
      "27/27 [==============================] - 18s 652ms/step\n",
      "\n",
      " Epoch 40/50\n",
      "27/27 [==============================] - 18s 652ms/step\n",
      "\n",
      " Epoch 41/50\n",
      "27/27 [==============================] - 18s 650ms/step\n",
      "\n",
      " Epoch 42/50\n",
      "27/27 [==============================] - 18s 650ms/step\n",
      "\n",
      " Epoch 43/50\n",
      "27/27 [==============================] - 18s 649ms/step\n",
      "\n",
      " Epoch 44/50\n",
      "27/27 [==============================] - 18s 650ms/step\n",
      "\n",
      " Epoch 45/50\n",
      "27/27 [==============================] - 18s 650ms/step\n",
      "\n",
      " Epoch 46/50\n",
      "27/27 [==============================] - 18s 650ms/step\n",
      "\n",
      " Epoch 47/50\n",
      "27/27 [==============================] - 18s 648ms/step\n",
      "\n",
      " Epoch 48/50\n",
      "27/27 [==============================] - 18s 651ms/step\n",
      "\n",
      " Epoch 49/50\n",
      "27/27 [==============================] - 18s 651ms/step\n",
      "\n",
      " Epoch 50/50\n",
      "27/27 [==============================] - 18s 650ms/step\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "train(train_data,EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch for test data\n",
    "test_input,test_val,y_true = testing_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 391ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.99988735e-01],\n",
       "       [2.15528307e-05],\n",
       "       [1.05179615e-05],\n",
       "       [6.85349377e-08],\n",
       "       [1.00000000e+00],\n",
       "       [9.99999881e-01],\n",
       "       [1.49907162e-08],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [6.14364531e-11],\n",
       "       [9.99999404e-01],\n",
       "       [4.78063526e-11],\n",
       "       [9.99504566e-01],\n",
       "       [1.12891718e-09],\n",
       "       [1.08146062e-10]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions \n",
    "pred = siamese_model_.predict([test_input, test_val])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post processing the results\n",
    "\n",
    "[1 if y_pred > 0.5 else 0 for y_pred in pred ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "rec = Recall()\n",
    "\n",
    "# Calculating\n",
    "rec.update_state(y_true,pred)\n",
    "rec.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "Pre = Precision()\n",
    "\n",
    "# Calculating\n",
    "Pre.update_state(y_true,pred)\n",
    "Pre.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[0])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "siamese_model_.save(\"siamesemodel.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model                                 \n",
    "custom_objects = {'L1DIST':L1DIST, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy}\n",
    "\n",
    "with tf.keras.saving.custom_object_scope(custom_objects):\n",
    "    model = tf.keras.models.load_model(\"siamesemodel.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_img (InputLayer)      [(None, 100, 100, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " validation_img (InputLayer  [(None, 100, 100, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)      (None, 4096)                 3896044   ['input_img[0][0]',           \n",
      "                                                          8          'validation_img[0][0]']      \n",
      "                                                                                                  \n",
      " l1dist_1 (L1DIST)           (None, 4096)                 0         ['embedding[0][0]',           \n",
      "                                                                     'embedding[1][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    4097      ['l1dist_1[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38964545 (148.64 MB)\n",
      "Trainable params: 38964545 (148.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    # Results array\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join(\"application_data\",\"verification_images\")):\n",
    "        input_img = preprocessing(os.path.join(\"application_data\",\"input_image\",\"input_image.jpg\"))\n",
    "        validation_img = preprocessing(os.path.join(\"application_data\",\"verification_images\",image))\n",
    "        \n",
    "        # Make predictions\n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "        \n",
    "    # Detection Threshold: Metric above which a prediciton is considered positive \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples \n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images'))) \n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    cv2.imshow('Verification', frame)\n",
    "    \n",
    "    # Verification\n",
    "    if cv2.waitKey(1) & 0xFF == ord('v'):\n",
    "        # save input img to application data --> input img folder\n",
    "        cv2.imwrite(os.path.join(\"application_data\",\"input_image\",\"input_image.jpg\"), frame)\n",
    "        \n",
    "        # Run verification\n",
    "        results, verified = verify(model,0.7,0.6)\n",
    "        print(verified)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
